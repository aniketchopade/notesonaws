# S3 
## S3 Basics
* Object Storage
    * Easy to use, pay only what you use
* Usage
    * Backup/ archive
    * Content distribution
    * Big data analysis
    * Static website hosting
    * Cloud native application hosting (???)
    * Disaster recovery
* Offers Storage classes
    * General purpose
    * Infrequent access
    * Archive
* Object vs File Storage
    * Server independent , accessed using HTTP verbs
    * No file / folder hierarchy
    * No partial data access (hello!! Range GET)
    * No worry about
        * Storage space , capacity planning
    * Highly durable / scalable
        * Auto replicated (durable)
        * Buckets are auto partitioned if traffic is High

## S3 Basics
* Buckets
    * Container
    * Name must be unique globally
    * 63 chars
    * 100 buckets / account
* Regions
    * Bucket created in region
    * Control your data's location 
        * Close to users
        * Far from primary (disster recovery)
* Objects
    * Can store any data; size uto 5tb
    * Object has data + medata
        * AWS doesnt care abt data (opaque)
        * medata
            * system
                * Modified timestamp, Hash
            * user
                * tags. only at create time
            * custom medata 
                * added after created
* Keys
    * Upto 1024 bytes UTF-8
* Object URL
    * <bucketname>.s3.amazonaws.com/keyname

* Durability and Availability
    * Durability - will i be able to access data in future. 11 9's
    * Availability - can i access my data now 99.99%
* Consistency
    * Data is replicated in aws, so eventually consistant
    * for new object put. read-after-write Consistency provided
    * all operations to SINGLE key are atomic. So you will never get partially updated data
* Access Control
    * IAM policy, Bucket Policy, ACLs
    * ACLs 
        * Legacy, used before IAM. >20kb
        * on the object level access.
    * Bucket
        * Bucket level upto 20kb
        * like IAM but it has EXPLICIT reference to principle 
* Static website hosting
## S3 advance features
* Delimiters
    * just for organize, manage
    * IAM , bucket policy understands this
        * Restrict or share data of subdirectories
    * AWS console, CLI understands this
* Storage classes
    * Standard
        * general purpose
        * long/short term of FREQUENTLY accessed data
    * Infrequent access (lower availability)
        * Data which is Infrequently accessed
        * Minimum 30days duration , 128kb min size
        * lower const
        * Same durability, latency, high throughput (so somewhat lower availability)
    * RRS (lower durability)
        * Data that can be derived
        * Lower durability
    * Glacier  (least availability , higher availability)
        * no real time access
        * response time ? 5 hours acceptable
        * once restore command issued; data is moved to RRS 
        * Free if 5% accessed
* Object lifecylce managemet
    * Data is hot, warm , cold
    * Store on standard, move to S3-IA after 30 days, after 3 months move to Glacier
* Encryption
    * Server side 
        * AWS S3
            * object encrypted with key which is again with master key
            * master key is rotated
            * data, key and master keys are kept in differnt host
        * AWS KMS
            * You manage these keys, keys are still generated by AWS
            * You can have audit to see usages and failures
        * AWS Customer provided Keys
            * You own keys , aws manage encryption process
    * Client side
        * You encrypt data and upload
        * Still use 
            * AWS Kms keys
            * Client master key
* Versioning
    * Accidental deletion, version id,
    * Cannot remove all versions , you can suspend it
* MFA delete
    * Another layer of authentication
* Presigned URLs
    * Protects from URL scappers
* Multipart upload
    * better network utilization
    * ability start, pause, resume for bigger data
* Range GET
    * same as Multipart download
    * specify range of bytes in GET
* Cross region replication
    * to reduce latency, Disaster recovery
    * asynchronously replicate objects in your s3 bucket
    * ACLs and metadata are also replicated
    * keep versioning on
    * set up IAM , amazon s3 service gets access rights to update another bucket
* Logging 
* Event notifications
    * As and when S3 object is updated, event triggerd and action can be taken
    * run workflows , trigger alert 
    * bucket level 
    * Deliverd via SQS, SNS or direct lambda trigger
## Amazon Glacier
* Low cost
* retreival time > 5 hours acceptable
* Archives
    * 40tb of data, 
    * id is generated, 
    * auto encrypted
* Vaults 
    * Container for Archives  
    * upto 1000 Vaults
* Vault locks
    * compliance
* Data retreival
    * free 5% of data retreival
    * more than 5% .. then AWS charges based on max. retrival rate
    * add policy to Vaults to minimize cost
